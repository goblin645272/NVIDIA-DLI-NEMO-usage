{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Build a Named Entity Recognizer\n",
    "### (NVIDIA NeMo v1.0)\n",
    "\n",
    "In this notebook, you'll build an NER (named entity recognition) application that finds disease names in medical disease abstracts. The model does not \"search\" for names from a list, but rather \"recognizes\" that certain words are disease references from the context of the language. \n",
    "\n",
    "**[3.1 Token Classification from the Command Line](#3.1-Token-Classification-from-the-Command-Line)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1 Data Input](#3.1.1-Data-Input)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1.1 IOB Tagging](#3.1.1.1-IOB-Tagging)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.2 Configuration File](#3.1.2-Configuration-File)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.3 Hydra-Enabled Python Scripts](#3.1.3-Hydra-Enabled-Python-Scripts)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.4 Exercise: Train the Model](#3.1.4-Exercise:-Train-the-Model)<br>\n",
    "**[3.2 Domain-Specific Training](#3.2-Domain-Specific-Training)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2.1 Visualize the Results with TensorBoard](#3.2.1-Visualize-the-Results-with-TensorBoard)<br>\n",
    "**[3.3 Evaluation](#3.3-Evaluation)**<br>\n",
    "**[3.4 Inference](#3.4-Inference)**<br>\n",
    "\n",
    "For the NER task, you'll follow the same basic steps as in the text classification task to build your project, train it, and test it.  This time, however, you'll train a classifier on the *domain-specific* BioMegatron language model.  BioMegatron is a [BERT](https://arxiv.org/abs/1810.04805)-like [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) model pre-trained on a large biomedical text corpus ([PubMed](https://pubmed.ncbi.nlm.nih.gov/) abstracts and full-text commercial use collection).  We can expect to have better performance compared to the general language models, because our disease dataset is from the same biomedical domain.\n",
    "\n",
    "There are some alternatives of BioMegatron, most notably [BioBERT](https://arxiv.org/abs/1901.08746). Compared to BioBERT, BioMegatron is larger by model size and pre-trained on larger text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.1 Token Classification from the Command Line\n",
    "The question we want to answer is:\n",
    "\n",
    "**Given sentences from medical abstracts, what diseases are mentioned?**<br>\n",
    "\n",
    "Recall the NLP models available with NeMo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mnemo/examples/nlp\u001b[00m\n",
      "├── \u001b[01;34mdialogue_state_tracking\u001b[00m\n",
      "├── \u001b[01;34mentity_linking\u001b[00m\n",
      "├── \u001b[01;34mglue_benchmark\u001b[00m\n",
      "├── \u001b[01;34minformation_retrieval\u001b[00m\n",
      "├── \u001b[01;34mintent_slot_classification\u001b[00m\n",
      "├── \u001b[01;34mlanguage_modeling\u001b[00m\n",
      "├── \u001b[01;34mmachine_translation\u001b[00m\n",
      "├── \u001b[01;34mquestion_answering\u001b[00m\n",
      "├── \u001b[01;34mtext2sparql\u001b[00m\n",
      "├── \u001b[01;34mtext_classification\u001b[00m\n",
      "└── \u001b[01;34mtoken_classification\u001b[00m\n",
      "\n",
      "11 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "!tree nemo/examples/nlp -L 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [token classification](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/token_classification.html) model for NER because we are classifying at the \"token\" level, in this case classifying words related to diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Data Input\n",
    "As we saw in the [1.0 Explore the Data](010_ExploreData.ipynb) notebook, the dataset for the NER project is made up of sentences with IOB tagging for disease names, where each word in a sentence is tagged as inside, outside, or the beginning of a named entity. \n",
    "\n",
    "The training text and label files are `text_train.txt` and `labels_train.txt`, respectively.  The validation and test files follow a similar naming pattern. Verify the location of the data files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.9M\n",
      "-rw-r--r-- 1 root   root   740K Aug  7 11:33 cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "-rw-r--r-- 1 root   root   4.3M Aug  7 11:33 cached_text_train.txt_BertTokenizer_64_28996_-1\n",
      "-rw-r--r-- 1 702112 10513  181K Jul 13  2020 dev.tsv\n",
      "-rw-r--r-- 1 702112 10513     5 Aug  7 11:35 label_ids.csv\n",
      "-rw-r--r-- 1 702112 10513    52 Jul 13  2020 label_stats.tsv\n",
      "-rw-r--r-- 1 702112 10513   48K Jul 13  2020 labels_dev.txt\n",
      "-rw-r--r-- 1 root   root     51 Aug  7 11:33 labels_dev_label_stats.tsv\n",
      "-rw-r--r-- 1 702112 10513   49K Jul 13  2020 labels_test.txt\n",
      "-rw-r--r-- 1 702112 10513  271K Jul 13  2020 labels_train.txt\n",
      "-rw-r--r-- 1 root   root     52 Aug  7 11:35 labels_train_label_stats.tsv\n",
      "-rw-r--r-- 1 root   root     43 Aug  7 11:33 labels_train_weights.p\n",
      "-rw-r--r-- 1 702112 10513  185K Jul 13  2020 test.tsv\n",
      "-rw-r--r-- 1 702112 10513  135K Jul 13  2020 text_dev.txt\n",
      "-rw-r--r-- 1 702112 10513  138K Jul 13  2020 text_test.txt\n",
      "-rw-r--r-- 1 702112 10513  758K Jul 13  2020 text_train.txt\n",
      "-rw-r--r-- 1 702112 10513 1023K Jul 13  2020 train.tsv\n",
      "-rw-r--r-- 1 702112 10513  1.2M Jul 13  2020 train_dev.tsv\n"
     ]
    }
   ],
   "source": [
    "NER3_DATA_DIR = '/dli/task/data/NCBI_ner-3'\n",
    "!ls -lh $NER3_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "text_test.txt sample\n",
      "*****\n",
      "Clustering of missense mutations in the ataxia - telangiectasia gene in a sporadic T - cell leukaemia . \n",
      "Ataxia - telangiectasia ( A - T ) is a recessive multi - system disorder caused by mutations in the ATM gene at 11q22 - q23 ( ref . 3 ) . \n",
      "The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A - T patients and has long been associated with chromosomal instability . \n",
      "\n",
      "*****\n",
      "labels_test.txt sample\n",
      "*****\n",
      "O O O O O O B I I O O O B I I I I O \n",
      "B I I O B I I O O O B I I I I O O O O O O O O O O O O O O O O O \n",
      "O O O B O O B I O O O O O B I I O O O O O O O O O O \n"
     ]
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "print(\"*****\\ntext_test.txt sample\\n*****\")\n",
    "!head -n 3 $NER3_DATA_DIR/text_test.txt\n",
    "print(\"\\n*****\\nlabels_test.txt sample\\n*****\")\n",
    "!head -n 3 $NER3_DATA_DIR/labels_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1.1 IOB Tagging\n",
    "Recall that the sentences and labels in the NER dataset map to each other with _inside, outside, beginning (IOB)_ tagging.\n",
    "This mechanism can be used in a general way for multiple named entity types:\n",
    "* B-{CHUNK_TYPE} – for the word in the Beginning chunk\n",
    "* I-{CHUNK_TYPE} – for words Inside the chunk\n",
    "* O – Outside any chunk\n",
    "\n",
    "In our case, we are only looking for \"disease\" as our entity (or chunk) type, so we don't need to identify beyond the three classes: I, O, and B.\n",
    "**Three classes**\n",
    "* B - Beginning of disease name\n",
    "* I - Inside word of disease name\n",
    "* O - Outside of all disease names\n",
    "\n",
    "```text\n",
    "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\n",
    "O              O  O    O O O         O  O   B           I         I    I      O          O  \n",
    "```\n",
    "\n",
    "These are defined in our `labels.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n",
      "B\n",
      "I"
     ]
    }
   ],
   "source": [
    "!head $NER3_DATA_DIR/label_ids.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were looking for two kinds of named entities, such as nouns and verbs in a parts-of-speech analysis, we would use a five-class IOB scheme:<br>\n",
    "**Five classes**\n",
    "* B-N - Beginning of noun word or phrase\n",
    "* I-N - Inside noun word or phrase\n",
    "* B-V - Beginning of verb word or phrase\n",
    "* I-V - Inside verb word or phrase\n",
    "* O   - Outside all nouns and verbs\n",
    "\n",
    "If you are intereested in learning more, take a look at [this paper](http://cs229.stanford.edu/proj2005/KrishnanGanapathy-NamedEntityRecognition.pdf) on the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NCBI_ner-3 disease data is in the correct format for token classification as described in the [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/token_classification.html#data-input-for-token-classification-model), so we are ready to look at the configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 Configuration File\n",
    "Look at more detail for the NeMo token classification directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/dli/task/nemo/examples/nlp/token_classification\u001b[00m\n",
      "├── \u001b[01;34mconf\u001b[00m\n",
      "│   ├── punctuation_capitalization_config.yaml\n",
      "│   └── token_classification_config.yaml\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── get_tatoeba_data.py\n",
      "│   ├── import_from_iob_format.py\n",
      "│   └── prepare_data_for_punctuation_capitalization.py\n",
      "├── punctuation_capitalization_evaluate.py\n",
      "├── punctuation_capitalization_train.py\n",
      "├── token_classification_evaluate.py\n",
      "└── token_classification_train.py\n",
      "\n",
      "2 directories, 9 files\n"
     ]
    }
   ],
   "source": [
    "TC_DIR = \"/dli/task/nemo/examples/nlp/token_classification\"\n",
    "!tree $TC_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config file for NER, `token_classification_config.yaml`, specifies model, training, and experiment management details, such as file locations, pretrained models, and hyperparameters.  This is the same general pattern used in the text classification configuration file.  We'll take a look at the details of each section using the `OmegaConf` tool introduced in the text classification project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_ids: null\n",
      "class_labels:\n",
      "  class_labels_file: label_ids.csv\n",
      "dataset:\n",
      "  data_dir: ???\n",
      "  class_balancing: null\n",
      "  max_seq_length: 128\n",
      "  pad_label: O\n",
      "  ignore_extra_tokens: false\n",
      "  ignore_start_end: false\n",
      "  use_cache: true\n",
      "  num_workers: 2\n",
      "  pin_memory: false\n",
      "  drop_last: false\n",
      "train_ds:\n",
      "  text_file: text_train.txt\n",
      "  labels_file: labels_train.txt\n",
      "  shuffle: true\n",
      "  num_samples: -1\n",
      "  batch_size: 64\n",
      "validation_ds:\n",
      "  text_file: text_dev.txt\n",
      "  labels_file: labels_dev.txt\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  batch_size: 64\n",
      "test_ds:\n",
      "  text_file: text_dev.txt\n",
      "  labels_file: labels_dev.txt\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  batch_size: 64\n",
      "tokenizer:\n",
      "  tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "  vocab_file: null\n",
      "  tokenizer_model: null\n",
      "  special_tokens: null\n",
      "language_model:\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  lm_checkpoint: null\n",
      "  config_file: null\n",
      "  config: null\n",
      "head:\n",
      "  num_fc_layers: 2\n",
      "  fc_dropout: 0.5\n",
      "  activation: relu\n",
      "  use_transformer_init: true\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 5.0e-05\n",
      "  weight_decay: 0.0\n",
      "  sched:\n",
      "    name: WarmupAnnealing\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "CONFIG_DIR = \"/dli/task/nemo/examples/nlp/token_classification/conf\"\n",
    "CONFIG_FILE = \"token_classification_config.yaml\"\n",
    "\n",
    "config = OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "\n",
    "# print the model section\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['megatron-bert-345m-uncased',\n",
       " 'megatron-bert-345m-cased',\n",
       " 'megatron-bert-uncased',\n",
       " 'megatron-bert-cased',\n",
       " 'biomegatron-bert-345m-uncased',\n",
       " 'biomegatron-bert-345m-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'bert-base-german-dbmdz-cased',\n",
       " 'bert-base-german-dbmdz-uncased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'wietsedv/bert-base-dutch-cased',\n",
       " 'distilbert-base-uncased',\n",
       " 'distilbert-base-uncased-distilled-squad',\n",
       " 'distilbert-base-cased',\n",
       " 'distilbert-base-cased-distilled-squad',\n",
       " 'distilbert-base-german-cased',\n",
       " 'distilbert-base-multilingual-cased',\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'roberta-base',\n",
       " 'roberta-large',\n",
       " 'roberta-large-mnli',\n",
       " 'distilroberta-base',\n",
       " 'roberta-base-openai-detector',\n",
       " 'roberta-large-openai-detector',\n",
       " 'albert-base-v1',\n",
       " 'albert-large-v1',\n",
       " 'albert-xlarge-v1',\n",
       " 'albert-xxlarge-v1',\n",
       " 'albert-base-v2',\n",
       " 'albert-large-v2',\n",
       " 'albert-xlarge-v2',\n",
       " 'albert-xxlarge-v2']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete list of supported BERT-like models\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "nemo_nlp.modules.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `model` section, a path for `dataset.data_dir` that contains all the data files is required.  The actual file names we are using already conform to the default values, so we don't need to override those.\n",
    "\n",
    "For our first try, we can override `language_model.pretrained_model_name` to `bert-base-cased`, so we can compare the results to the domain-specific `biomegatron-bert-345m-cased` in another experiment.  Since we will need to conserve memory space to run BioMegatron, we will go ahead and reduce `dataset.max_seq_length` and the `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 5\n",
      "max_steps: null\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "amp_level: O0\n",
      "precision: 16\n",
      "accelerator: ddp\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "resume_from_checkpoint: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the trainer section\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficiency, we can set the `amp_level` to 'O1'.  Since the language models we are going to compare are large and take a long time to run, we will override the `max_epochs` to a small number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_dir: null\n",
      "name: token_classification_model\n",
      "create_tensorboard_logger: true\n",
      "create_checkpoint_callback: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the experiment manager section\n",
    "print(OmegaConf.to_yaml(config.exp_manager))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to change the `exp_manger` default settings for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.1.3 Hydra-Enabled Python Scripts\n",
    "The Python scripts, `token_classification_train.py` and `token_evaluate.py`, encapsulate everything needed to run a token classification experiment defined by the configuration file.  Training and evaluation are expected to be run separately in this case.  As with text classification, both scripts employ Facebook's [Hydra](https://hydra.cc/) tool for configuration management, which allows the entire experiment to be run from the command line, overriding config file values as needed.\n",
    "\n",
    "To recap, the parameters we need to change or override are:\n",
    "\n",
    "* `model.language_model.pretrained_model_name`: set to 'bert-base-cased'\n",
    "* `model.dataset.data_dir`: set to /dli/task/data/NCBI_ner-3\n",
    "* `model.dataset.max_seq_length`: 64\n",
    "* `model.train_ds.batch_size`: set to 32\n",
    "* `model.val_ds.batch_size`: set to 32\n",
    "* `model.test_ds.batch_size`: set to 32\n",
    "* `trainer.amp_level`: set to \"O1\"\n",
    "* `trainer.max_epochs`: set to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.4 Exercise: Train the Model\n",
    "Run the training script, `token_classification_train.py` just as you ran similar experiments in text classification notebook.   \n",
    "\n",
    "The new values for overrides are provided for you in the cell below.  Add the command with appropriate overrides and run the cell.  If you get stuck, refer to the [solution](solutions/ex3.1.4.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2021-08-07 12:32:29 exp_manager:216] Experiments will be logged at /dli/task/nemo_experiments/token_classification_model/2021-08-07_12-32-29\n",
      "[NeMo I 2021-08-07 12:32:29 exp_manager:563] TensorboardLogger has been set up\n",
      "[NeMo I 2021-08-07 12:32:29 token_classification_train:109] Config: pretrained_model: null\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 3\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O1\n",
      "      precision: 16\n",
      "      accelerator: ddp\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: token_classification_model\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    model:\n",
      "      label_ids: null\n",
      "      class_labels:\n",
      "        class_labels_file: label_ids.csv\n",
      "      dataset:\n",
      "        data_dir: /dli/task/data/NCBI_ner-3\n",
      "        class_balancing: null\n",
      "        max_seq_length: 64\n",
      "        pad_label: O\n",
      "        ignore_extra_tokens: false\n",
      "        ignore_start_end: false\n",
      "        use_cache: true\n",
      "        num_workers: 2\n",
      "        pin_memory: false\n",
      "        drop_last: false\n",
      "      train_ds:\n",
      "        text_file: text_train.txt\n",
      "        labels_file: labels_train.txt\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      validation_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      test_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-cased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      head:\n",
      "        num_fc_layers: 2\n",
      "        fc_dropout: 0.5\n",
      "        activation: relu\n",
      "        use_transformer_init: true\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 5.0e-05\n",
      "        weight_decay: 0.0\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "    \n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2021-08-07 12:32:29 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_train.txt\n",
      "[NeMo I 2021-08-07 12:32:29 token_classification_utils:90] Labels mapping {'O': 0, 'B': 1, 'I': 2} saved to : /dli/task/data/NCBI_ner-3/label_ids.csv\n",
      "[NeMo I 2021-08-07 12:32:29 token_classification_utils:99] Three most popular labels in /dli/task/data/NCBI_ner-3/labels_train.txt:\n",
      "[NeMo I 2021-08-07 12:32:29 data_preprocessing:135] label: 0, 124452 out of 135701 (91.71%).\n",
      "[NeMo I 2021-08-07 12:32:29 data_preprocessing:135] label: 2, 6115 out of 135701 (4.51%).\n",
      "[NeMo I 2021-08-07 12:32:29 data_preprocessing:135] label: 1, 5134 out of 135701 (3.78%).\n",
      "[NeMo I 2021-08-07 12:32:29 token_classification_utils:101] Total labels: 135701. Label frequencies - {0: 124452, 2: 6115, 1: 5134}\n",
      "[NeMo I 2021-08-07 12:32:29 token_classification_utils:107] Class weights restored from /dli/task/data/NCBI_ner-3/labels_train_weights.p\n",
      "[NeMo W 2021-08-07 12:32:29 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo I 2021-08-07 12:32:30 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_train.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo I 2021-08-07 12:32:30 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2021-08-07 12:32:30 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2021-08-07 12:32:30 token_classification_utils:96] /dli/task/data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2021-08-07 12:32:30 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo I 2021-08-07 12:32:30 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2021-08-07 12:32:30 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2021-08-07 12:32:30 token_classification_utils:96] /dli/task/data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2021-08-07 12:32:30 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo W 2021-08-07 12:32:30 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertEncoder: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2021-08-07 12:32:33 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 5e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2021-08-07 12:32:33 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7fe60e9eb6d0>\" \n",
      "    will be used during training (effective maximum steps = 510) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 510\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "INFO:root:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | bert_model            | BertEncoder          | 108 M \n",
      "1 | classifier            | TokenClassifier      | 592 K \n",
      "2 | loss                  | CrossEntropyLoss     | 0     \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "435.613   Total estimated model params size (MB)\n",
      "[NeMo W 2021-08-07 12:32:35 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Validation sanity check:  50%|██████████          | 1/2 [00:00<00:00,  1.62it/s][NeMo I 2021-08-07 12:32:36 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         91.33      10.63      19.04       1487\n",
      "    B (label_id: 1)                                          2.07      61.90       4.00         42\n",
      "    I (label_id: 2)                                          8.99      20.25      12.45         79\n",
      "    -------------------\n",
      "    micro avg                                               12.44      12.44      12.44       1608\n",
      "    macro avg                                               34.13      30.93      11.83       1608\n",
      "    weighted avg                                            84.95      12.44      18.32       1608\n",
      "    \n",
      "[NeMo W 2021-08-07 12:32:36 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Epoch 0:  85%|▊| 170/199 [00:14<00:02, 11.80it/s, loss=0.0797, v_num=2-29, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  86%|▊| 172/199 [00:14<00:02, 11.80it/s, loss=0.0797, v_num=2-29, val_l\u001b[A\n",
      "Epoch 0:  88%|▉| 176/199 [00:14<00:01, 11.98it/s, loss=0.0797, v_num=2-29, val_l\u001b[A\n",
      "Epoch 0:  90%|▉| 180/199 [00:14<00:01, 12.16it/s, loss=0.0797, v_num=2-29, val_l\u001b[A\n",
      "Epoch 0:  92%|▉| 184/199 [00:14<00:01, 12.34it/s, loss=0.0797, v_num=2-29, val_l\u001b[A\n",
      "Epoch 0:  94%|▉| 188/199 [00:15<00:00, 12.51it/s, loss=0.0797, v_num=2-29, val_l\u001b[A\n",
      "Epoch 0:  96%|▉| 192/199 [00:15<00:00, 12.68it/s, loss=0.0797, v_num=2-29, val_l\u001b[A\n",
      "Epoch 0:  98%|▉| 196/199 [00:15<00:00, 12.85it/s, loss=0.0797, v_num=2-29, val_l\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 34.40it/s]\u001b[A[NeMo I 2021-08-07 12:32:51 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.46      99.02      99.24      21648\n",
      "    B (label_id: 1)                                         80.71      89.21      84.74        769\n",
      "    I (label_id: 2)                                         89.52      90.77      90.14       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.32      98.32      98.32      23490\n",
      "    macro avg                                               89.90      93.00      91.38      23490\n",
      "    weighted avg                                            98.39      98.32      98.35      23490\n",
      "    \n",
      "Epoch 0: 100%|█| 199/199 [00:15<00:00, 12.91it/s, loss=0.0797, v_num=2-29, val_l\n",
      "                                                                                \u001b[AEpoch 0, global step 169: val_loss reached 0.07138 (best 0.07138), saving model to \"/dli/task/nemo_experiments/token_classification_model/2021-08-07_12-32-29/checkpoints/token_classification_model--val_loss=0.07-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  85%|▊| 170/199 [00:14<00:02, 11.41it/s, loss=0.0423, v_num=2-29, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  86%|▊| 172/199 [00:15<00:02, 11.42it/s, loss=0.0423, v_num=2-29, val_l\u001b[A\n",
      "Epoch 1:  88%|▉| 176/199 [00:15<00:01, 11.59it/s, loss=0.0423, v_num=2-29, val_l\u001b[A\n",
      "Epoch 1:  90%|▉| 180/199 [00:15<00:01, 11.77it/s, loss=0.0423, v_num=2-29, val_l\u001b[A\n",
      "Epoch 1:  92%|▉| 184/199 [00:15<00:01, 11.94it/s, loss=0.0423, v_num=2-29, val_l\u001b[A\n",
      "Epoch 1:  94%|▉| 188/199 [00:15<00:00, 12.11it/s, loss=0.0423, v_num=2-29, val_l\u001b[A\n",
      "Epoch 1:  96%|▉| 192/199 [00:15<00:00, 12.28it/s, loss=0.0423, v_num=2-29, val_l\u001b[A\n",
      "Epoch 1:  98%|▉| 196/199 [00:15<00:00, 12.44it/s, loss=0.0423, v_num=2-29, val_l\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 34.00it/s]\u001b[A[NeMo I 2021-08-07 12:33:13 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.34      99.36      99.35      21648\n",
      "    B (label_id: 1)                                         84.33      89.60      86.89        769\n",
      "    I (label_id: 2)                                         93.14      88.63      90.83       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.55      98.55      98.55      23490\n",
      "    macro avg                                               92.27      92.53      92.35      23490\n",
      "    weighted avg                                            98.57      98.55      98.55      23490\n",
      "    \n",
      "Epoch 1: 100%|█| 199/199 [00:15<00:00, 12.50it/s, loss=0.0423, v_num=2-29, val_l\n",
      "                                                                                \u001b[AEpoch 1, global step 339: val_loss reached 0.06936 (best 0.06936), saving model to \"/dli/task/nemo_experiments/token_classification_model/2021-08-07_12-32-29/checkpoints/token_classification_model--val_loss=0.07-epoch=1.ckpt\" as top 3\n",
      "Epoch 2:  85%|▊| 170/199 [00:14<00:02, 11.62it/s, loss=0.0269, v_num=2-29, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  86%|▊| 172/199 [00:14<00:02, 11.63it/s, loss=0.0269, v_num=2-29, val_l\u001b[A\n",
      "Epoch 2:  88%|▉| 176/199 [00:14<00:01, 11.80it/s, loss=0.0269, v_num=2-29, val_l\u001b[A\n",
      "Epoch 2:  90%|▉| 180/199 [00:15<00:01, 11.98it/s, loss=0.0269, v_num=2-29, val_l\u001b[A\n",
      "Epoch 2:  92%|▉| 184/199 [00:15<00:01, 12.15it/s, loss=0.0269, v_num=2-29, val_l\u001b[A\n",
      "Epoch 2:  94%|▉| 188/199 [00:15<00:00, 12.31it/s, loss=0.0269, v_num=2-29, val_l\u001b[A\n",
      "Epoch 2:  96%|▉| 192/199 [00:15<00:00, 12.47it/s, loss=0.0269, v_num=2-29, val_l\u001b[A\n",
      "Epoch 2:  98%|▉| 196/199 [00:15<00:00, 12.63it/s, loss=0.0269, v_num=2-29, val_l\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 32.03it/s]\u001b[A[NeMo I 2021-08-07 12:33:35 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.32      99.39      99.35      21648\n",
      "    B (label_id: 1)                                         85.46      88.69      87.05        769\n",
      "    I (label_id: 2)                                         92.32      88.54      90.39       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.54      98.54      98.54      23490\n",
      "    macro avg                                               92.37      92.20      92.26      23490\n",
      "    weighted avg                                            98.54      98.54      98.54      23490\n",
      "    \n",
      "Epoch 2: 100%|█| 199/199 [00:15<00:00, 12.69it/s, loss=0.0269, v_num=2-29, val_l\n",
      "                                                                                \u001b[AEpoch 2, global step 509: val_loss reached 0.07379 (best 0.06936), saving model to \"/dli/task/nemo_experiments/token_classification_model/2021-08-07_12-32-29/checkpoints/token_classification_model--val_loss=0.07-epoch=2.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n",
      "Epoch 2: 100%|█| 199/199 [00:22<00:00,  9.02it/s, loss=0.0269, v_num=2-29, val_l\n",
      "[NeMo W 2021-08-07 12:33:42 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "CPU times: user 1.98 s, sys: 532 ms, total: 2.52 s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The training takes about 2 minutes to run\n",
    "   \n",
    "TOKEN_DIR = \"/dli/task/nemo/examples/nlp/token_classification\"\n",
    "\n",
    "# set the values we want to override\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "DATA_DIR = '/dli/task/data/NCBI_ner-3'\n",
    "MAX_SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 32\n",
    "AMP_LEVEL = 'O1'\n",
    "MAX_EPOCHS = 3\n",
    "\n",
    "# Override the config values in the command line\n",
    "# FIXME\n",
    "!python $TOKEN_DIR/token_classification_train.py \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        model.dataset.data_dir=$DATA_DIR \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.test_ds.batch_size=$BATCH_SIZE \\\n",
    "        trainer.amp_level=$AMP_LEVEL \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How were the results?  Your log should have included something like:\n",
    "\n",
    "```\n",
    "    label                                                precision    recall       f1           support   \n",
    "    O (label_id: 0)                                         99.34      99.35      99.34      21648\n",
    "    B (label_id: 1)                                         85.86      89.21      87.50        769\n",
    "    I (label_id: 2)                                         91.74      89.00      90.35       1073\n",
    "    -------------------\n",
    "    micro avg                                               98.54      98.54      98.54      23490\n",
    "    macro avg                                               92.31      92.52      92.40      23490\n",
    "    weighted avg                                            98.55      98.54      98.55      23490\n",
    "    \n",
    "Epoch 2: 100%|█| 199/199 [00:15<00:00, 12.45it/s, loss=0.0251, v_num=4-43, val_l\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.2 Domain-Specific Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try another experiment, this time overriding the `model.language_model.pretrained_model_name` with `biomegatron-bert-345m-cased`.  This is a large model with 345 million parameter.  Therefore, it takes longer to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2021-08-07 12:34:13 exp_manager:216] Experiments will be logged at /dli/task/nemo_experiments/token_classification_model/2021-08-07_12-34-13\n",
      "[NeMo I 2021-08-07 12:34:13 exp_manager:563] TensorboardLogger has been set up\n",
      "[NeMo I 2021-08-07 12:34:13 token_classification_train:109] Config: pretrained_model: null\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 3\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O1\n",
      "      precision: 16\n",
      "      accelerator: ddp\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: token_classification_model\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    model:\n",
      "      label_ids: null\n",
      "      class_labels:\n",
      "        class_labels_file: label_ids.csv\n",
      "      dataset:\n",
      "        data_dir: /dli/task/data/NCBI_ner-3\n",
      "        class_balancing: null\n",
      "        max_seq_length: 64\n",
      "        pad_label: O\n",
      "        ignore_extra_tokens: false\n",
      "        ignore_start_end: false\n",
      "        use_cache: true\n",
      "        num_workers: 2\n",
      "        pin_memory: false\n",
      "        drop_last: false\n",
      "      train_ds:\n",
      "        text_file: text_train.txt\n",
      "        labels_file: labels_train.txt\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      validation_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      test_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: biomegatron-bert-345m-cased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      head:\n",
      "        num_fc_layers: 2\n",
      "        fc_dropout: 0.5\n",
      "        activation: relu\n",
      "        use_transformer_init: true\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 5.0e-05\n",
      "        weight_decay: 0.0\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "    \n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_train.txt\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_utils:90] Labels mapping {'O': 0, 'B': 1, 'I': 2} saved to : /dli/task/data/NCBI_ner-3/label_ids.csv\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_utils:99] Three most popular labels in /dli/task/data/NCBI_ner-3/labels_train.txt:\n",
      "[NeMo I 2021-08-07 12:34:14 data_preprocessing:135] label: 0, 124452 out of 135701 (91.71%).\n",
      "[NeMo I 2021-08-07 12:34:14 data_preprocessing:135] label: 2, 6115 out of 135701 (4.51%).\n",
      "[NeMo I 2021-08-07 12:34:14 data_preprocessing:135] label: 1, 5134 out of 135701 (3.78%).\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_utils:101] Total labels: 135701. Label frequencies - {0: 124452, 2: 6115, 1: 5134}\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_utils:107] Class weights restored from /dli/task/data/NCBI_ner-3/labels_train_weights.p\n",
      "[NeMo W 2021-08-07 12:34:14 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_train.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_utils:96] /dli/task/data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_utils:96] /dli/task/data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2021-08-07 12:34:14 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo W 2021-08-07 12:34:14 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "setting global batch size to 1\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_load ....................................... None\n",
      "  bias_dropout_fusion ............................. False\n",
      "  bias_gelu_fusion ................................ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... False\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... infer\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  DDP_impl ........................................ local\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  faiss_use_gpu ................................... False\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_allreduce .................................. False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 1\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... True\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_interval .................................... 100\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. None\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  max_position_embeddings ......................... 512\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... False\n",
      "  no_load_rng ..................................... False\n",
      "  no_save_optim ................................... False\n",
      "  no_save_rng ..................................... False\n",
      "  num_attention_heads ............................. 16\n",
      "  num_layers ...................................... 24\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... True\n",
      "  openai_gelu ..................................... False\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  report_topk_accuracies .......................... []\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scaled_masked_softmax_fusion .................... False\n",
      "  scaled_upper_triang_masked_softmax_fusion ....... False\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... None\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 969, 30, 1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. BertWordPieceLowerCase\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_cpu_initialization .......................... False\n",
      "  use_one_sent_docs ............................... False\n",
      "  vocab_file ...................................... /root/.cache/huggingface/nemo_nlp_tmp/c6c377d258d448da6d9259a4c9660f26/tokenizer.vocab_file\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building BertWordPieceLowerCase tokenizer ...\n",
      " > padded vocab (size: 28996) with 60 dummy tokens (new size: 29056)\n",
      "[NeMo I 2021-08-07 12:34:14 megatron_bert:109] Megatron-lm argparse args: Namespace(DDP_impl='local', adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, adlr_autoresume=False, adlr_autoresume_interval=1000, apply_query_key_layer_scaling=True, apply_residual_connection_post_layernorm=False, attention_dropout=0.1, attention_softmax_in_fp32=False, bert_load=None, bias_dropout_fusion=False, bias_gelu_fusion=False, block_data_path=None, checkpoint_activations=False, checkpoint_num_layers=1, clip_grad=1.0, consumed_train_samples=0, consumed_valid_samples=0, data_impl='infer', data_parallel_size=1, data_path=None, distribute_checkpointed_activations=False, distributed_backend='nccl', eod_mask_loss=False, eval_interval=1000, eval_iters=100, exit_duration_in_mins=None, exit_interval=None, faiss_use_gpu=False, finetune=False, fp16=False, fp16_lm_cross_entropy=False, fp32_allreduce=False, fp32_residual_connection=False, global_batch_size=1, hidden_dropout=0.1, hidden_size=1024, hysteresis=2, ict_head_size=None, ict_load=None, indexer_batch_size=128, indexer_log_interval=1000, init_method_std=0.02, initial_loss_scale=4294967296, layernorm_epsilon=1e-05, lazy_mpu_init=True, load=None, local_rank=None, log_interval=100, loss_scale=None, loss_scale_window=1000, lr=None, lr_decay_iters=None, lr_decay_samples=None, lr_decay_style='linear', lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, make_vocab_size_divisible_by=128, mask_prob=0.15, max_position_embeddings=512, merge_file=None, micro_batch_size=1, min_loss_scale=1.0, min_lr=0.0, mmap_warmup=False, no_load_optim=False, no_load_rng=False, no_save_optim=False, no_save_rng=False, num_attention_heads=16, num_layers=24, num_workers=2, onnx_safe=True, openai_gelu=False, override_lr_scheduler=False, padded_vocab_size=29056, params_dtype=torch.float32, pipeline_model_parallel_size=1, query_in_block_prob=0.1, rampup_batch_size=None, rank=0, report_topk_accuracies=[], reset_attention_mask=False, reset_position_ids=False, save=None, save_interval=None, scaled_masked_softmax_fusion=False, scaled_upper_triang_masked_softmax_fusion=False, seed=1234, seq_length=None, short_seq_prob=0.1, split='969, 30, 1', tensor_model_parallel_size=1, tensorboard_dir=None, titles_data_path=None, tokenizer_type='BertWordPieceLowerCase', train_iters=None, train_samples=None, use_checkpoint_lr_scheduler=False, use_cpu_initialization=True, use_one_sent_docs=False, vocab_file='/root/.cache/huggingface/nemo_nlp_tmp/c6c377d258d448da6d9259a4c9660f26/tokenizer.vocab_file', weight_decay=0.01, world_size=1)\n",
      "[NeMo W 2021-08-07 12:34:18 megatron_bert:185] Megatron-lm checkpoint version not found. Setting checkpoint_version to 0.\n",
      "[NeMo I 2021-08-07 12:34:18 megatron_bert:192] Checkpoint loaded from from /root/.cache/torch/megatron/biomegatron-bert-345m-cased\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2021-08-07 12:34:18 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 5e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2021-08-07 12:34:18 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7fa20d6441c0>\" \n",
      "    will be used during training (effective maximum steps = 510) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 510\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "INFO:root:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | bert_model            | MegatronBertEncoder  | 332 M \n",
      "1 | classifier            | TokenClassifier      | 1.1 M \n",
      "2 | loss                  | CrossEntropyLoss     | 0     \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "333 M     Trainable params\n",
      "0         Non-trainable params\n",
      "333 M     Total params\n",
      "1,334.575 Total estimated model params size (MB)\n",
      "[NeMo W 2021-08-07 12:34:20 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Validation sanity check:   0%|                            | 0/2 [00:00<?, ?it/s]torch distributed is already initialized, skipping initialization ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "INFO:root:Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "INFO:root:Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "INFO:root:Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "INFO:root:Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "INFO:root:Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "Validation sanity check:  50%|██████████          | 1/2 [00:01<00:01,  1.09s/it][NeMo I 2021-08-07 12:34:21 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         94.52      26.70      41.64       1487\n",
      "    B (label_id: 1)                                          3.30      26.19       5.87         42\n",
      "    I (label_id: 2)                                          5.85      63.29      10.71         79\n",
      "    -------------------\n",
      "    micro avg                                               28.48      28.48      28.48       1608\n",
      "    macro avg                                               34.56      38.73      19.40       1608\n",
      "    weighted avg                                            87.78      28.48      39.18       1608\n",
      "    \n",
      "[NeMo W 2021-08-07 12:34:21 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Epoch 0:   0%|                                          | 0/199 [00:00<?, ?it/s][W reducer.cpp:1060] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance. If your model indeed never has any unused parameters, consider turning this flag off. Note that this warning may be a false positive your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 0:  85%|▊| 170/199 [00:33<00:05,  5.14it/s, loss=0.0716, v_num=4-13, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  86%|▊| 172/199 [00:33<00:05,  5.16it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  87%|▊| 174/199 [00:33<00:04,  5.21it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  88%|▉| 176/199 [00:33<00:04,  5.25it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  89%|▉| 178/199 [00:33<00:03,  5.29it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  90%|▉| 180/199 [00:33<00:03,  5.34it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  91%|▉| 182/199 [00:33<00:03,  5.38it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  92%|▉| 184/199 [00:33<00:02,  5.42it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  93%|▉| 186/199 [00:34<00:02,  5.46it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  94%|▉| 188/199 [00:34<00:01,  5.51it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  95%|▉| 190/199 [00:34<00:01,  5.55it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  96%|▉| 192/199 [00:34<00:01,  5.59it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  97%|▉| 194/199 [00:34<00:00,  5.63it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  98%|▉| 196/199 [00:34<00:00,  5.67it/s, loss=0.0716, v_num=4-13, val_l\u001b[A\n",
      "Epoch 0:  99%|▉| 198/199 [00:34<00:00,  5.71it/s, loss=0.0716, v_num=4-13, val_l\u001b[A[NeMo I 2021-08-07 12:34:56 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.37      99.38      99.37      21648\n",
      "    B (label_id: 1)                                         86.67      91.29      88.92        769\n",
      "    I (label_id: 2)                                         92.32      88.54      90.39       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.62      98.62      98.62      23490\n",
      "    macro avg                                               92.79      93.07      92.89      23490\n",
      "    weighted avg                                            98.63      98.62      98.62      23490\n",
      "    \n",
      "Epoch 0: 100%|█| 199/199 [00:34<00:00,  5.71it/s, loss=0.0716, v_num=4-13, val_l\n",
      "                                                                                \u001b[AEpoch 0, global step 169: val_loss reached 0.05910 (best 0.05910), saving model to \"/dli/task/nemo_experiments/token_classification_model/2021-08-07_12-34-13/checkpoints/token_classification_model--val_loss=0.06-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  85%|▊| 170/199 [00:32<00:05,  5.28it/s, loss=0.031, v_num=4-13, val_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  86%|▊| 172/199 [00:32<00:05,  5.30it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  87%|▊| 174/199 [00:32<00:04,  5.34it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  88%|▉| 176/199 [00:32<00:04,  5.39it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  89%|▉| 178/199 [00:32<00:03,  5.43it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  90%|▉| 180/199 [00:32<00:03,  5.48it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  91%|▉| 182/199 [00:32<00:03,  5.52it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  92%|▉| 184/199 [00:33<00:02,  5.56it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  93%|▉| 186/199 [00:33<00:02,  5.60it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  94%|▉| 188/199 [00:33<00:01,  5.65it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  95%|▉| 190/199 [00:33<00:01,  5.69it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  96%|▉| 192/199 [00:33<00:01,  5.73it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  97%|▉| 194/199 [00:33<00:00,  5.77it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  98%|▉| 196/199 [00:33<00:00,  5.81it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Epoch 1:  99%|▉| 198/199 [00:33<00:00,  5.85it/s, loss=0.031, v_num=4-13, val_lo\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:01<00:00, 19.28it/s]\u001b[A[NeMo I 2021-08-07 12:35:51 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.29      99.47      99.38      21648\n",
      "    B (label_id: 1)                                         87.28      90.12      88.68        769\n",
      "    I (label_id: 2)                                         93.46      87.88      90.59       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.64      98.64      98.64      23490\n",
      "    macro avg                                               93.34      92.49      92.88      23490\n",
      "    weighted avg                                            98.63      98.64      98.63      23490\n",
      "    \n",
      "Epoch 1: 100%|█| 199/199 [00:37<00:00,  5.29it/s, loss=0.031, v_num=4-13, val_lo\n",
      "                                                                                \u001b[AEpoch 1, global step 339: val_loss reached 0.05916 (best 0.05910), saving model to \"/dli/task/nemo_experiments/token_classification_model/2021-08-07_12-34-13/checkpoints/token_classification_model--val_loss=0.06-epoch=1.ckpt\" as top 3\n",
      "Epoch 2:  85%|▊| 170/199 [00:32<00:05,  5.28it/s, loss=0.0153, v_num=4-13, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  86%|▊| 172/199 [00:32<00:05,  5.31it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  87%|▊| 174/199 [00:32<00:04,  5.35it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  88%|▉| 176/199 [00:32<00:04,  5.40it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  89%|▉| 178/199 [00:32<00:03,  5.44it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  90%|▉| 180/199 [00:32<00:03,  5.48it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  91%|▉| 182/199 [00:32<00:03,  5.53it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  92%|▉| 184/199 [00:33<00:02,  5.57it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  93%|▉| 186/199 [00:33<00:02,  5.61it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  94%|▉| 188/199 [00:33<00:01,  5.65it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  95%|▉| 190/199 [00:33<00:01,  5.70it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  96%|▉| 192/199 [00:33<00:01,  5.74it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  97%|▉| 194/199 [00:33<00:00,  5.78it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  98%|▉| 196/199 [00:33<00:00,  5.82it/s, loss=0.0153, v_num=4-13, val_l\u001b[A\n",
      "Epoch 2:  99%|▉| 198/199 [00:33<00:00,  5.86it/s, loss=0.0153, v_num=4-13, val_l\u001b[A[NeMo I 2021-08-07 12:36:43 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.33      99.44      99.38      21648\n",
      "    B (label_id: 1)                                         86.30      90.12      88.17        769\n",
      "    I (label_id: 2)                                         93.41      88.44      90.86       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.63      98.63      98.63      23490\n",
      "    macro avg                                               93.01      92.67      92.80      23490\n",
      "    weighted avg                                            98.63      98.63      98.63      23490\n",
      "    \n",
      "Epoch 2: 100%|█| 199/199 [00:33<00:00,  5.86it/s, loss=0.0153, v_num=4-13, val_l\n",
      "                                                                                \u001b[AEpoch 2, global step 509: val_loss reached 0.06670 (best 0.05910), saving model to \"/dli/task/nemo_experiments/token_classification_model/2021-08-07_12-34-13/checkpoints/token_classification_model--val_loss=0.07-epoch=2.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n",
      "Epoch 2: 100%|█| 199/199 [00:51<00:00,  3.89it/s, loss=0.0153, v_num=4-13, val_l\n",
      "[NeMo W 2021-08-07 12:37:00 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "CPU times: user 4.53 s, sys: 1.4 s, total: 5.93 s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The training takes about 5-6 minutes to run\n",
    "   \n",
    "TOKEN_DIR = \"/dli/task/nemo/examples/nlp/token_classification\"\n",
    "\n",
    "# set the values we want to override\n",
    "PRETRAINED_MODEL_NAME = 'biomegatron-bert-345m-cased'\n",
    "DATA_DIR = '/dli/task/data/NCBI_ner-3'\n",
    "MAX_SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 32\n",
    "AMP_LEVEL = 'O1'\n",
    "MAX_EPOCHS = 3\n",
    "\n",
    "# Override the config values in the command line\n",
    "!python $TOKEN_DIR/token_classification_train.py \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        model.dataset.data_dir=$DATA_DIR \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.test_ds.batch_size=$BATCH_SIZE \\\n",
    "        trainer.amp_level=$AMP_LEVEL \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Visualize the Results with TensorBoard\n",
    "The [experiment manager](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html?highlight=tensorboard#experiment-manager) saves results for viewing with TensorBoard. Execute the following cell to create a link to TensorBoard for your instance, then click on the link to open Tensorboard in a tab on your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "const href = window.location.hostname +'/tensorboard/';\n",
       "let a = document.createElement('a');\n",
       "let link = document.createTextNode('Open Tensorboard!');\n",
       "a.appendChild(link);\n",
       "a.href = \"http://\" + href;\n",
       "a.style.color = \"navy\"\n",
       "a.target = \"_blank\"\n",
       "element.append(a);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the performance of the models you've run, select the \"f1\" scaler.  You can see all the models compared together or select individual models for comparison.  In this example comparison, five epochs were run.  The orange line shows results from the `bert-base-cased` model and the blue line is the `biomegatron-bert-345m-cased` model.  The BioMegatron model does quite well very quickly, as it is better able to discern the disease names. It still has a slightly higher f1 after five epochs. The model you choose for your own project depends on your constraints in memory, time, and performance requirements.  Note that your results may vary from the example due to randomness in the learning algorithm.\n",
    "\n",
    "<img src=\"images/tensorboard_02.png\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart the kernel\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model over the test set, we must specify the location of the `.nemo` trained model. Each experiment runs results in a time-stamped directory under `nemo_experiments`.  If we drill down, we can find the `checkpoints` folder where the final `token_classification_model.nemo` resides. In the next cell, a bit of Python logic is used to capture a list of models, and identify the latest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest model is \n",
      "nemo_experiments/token_classification_model/2021-08-07_12-34-13/checkpoints/token_classification_model.nemo\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "nemo_model_paths = glob.glob('nemo_experiments/token_classification_model/*/checkpoints/*.nemo')\n",
    "\n",
    "# Sort newest first\n",
    "nemo_model_paths.sort(reverse=True)\n",
    "print(\"The latest model is \\n{}\".format(nemo_model_paths[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways to run an evaluation over the test set:\n",
    "1. Execute `token_classification_evaluate.py` with the same overrides, plus an override for the `pretrained_model`, which must be in `.nemo` format.\n",
    "\n",
    "```text\n",
    "   !python $TOKEN_DIR/token_classification_evaluate.py \\\n",
    "        model.dataset.data_dir=$DATA_DIR \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.test_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        pretrained_model=$LATEST_MODEL\n",
    "```\n",
    "        \n",
    "2. Instantiate the model by restoring the trained model checkpoint and execute a NeMo method to evaluate the test set.<br>\n",
    "   This is the method we will step through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2021-08-07 12:40:53 modelPT:137] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    text_file: text_train.txt\n",
      "    labels_file: labels_train.txt\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    batch_size: 32\n",
      "    \n",
      "[NeMo W 2021-08-07 12:40:53 modelPT:144] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    batch_size: 32\n",
      "    \n",
      "[NeMo W 2021-08-07 12:40:53 modelPT:151] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    batch_size: 32\n",
      "    \n",
      "[NeMo W 2021-08-07 12:40:53 modelPT:1198] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2021-08-07 12:40:53 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo W 2021-08-07 12:40:53 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "setting global batch size to 1\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_load ....................................... None\n",
      "  bias_dropout_fusion ............................. False\n",
      "  bias_gelu_fusion ................................ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... False\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... infer\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  DDP_impl ........................................ local\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  faiss_use_gpu ................................... False\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_allreduce .................................. False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 1\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... True\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_interval .................................... 100\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. None\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  max_position_embeddings ......................... 512\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... False\n",
      "  no_load_rng ..................................... False\n",
      "  no_save_optim ................................... False\n",
      "  no_save_rng ..................................... False\n",
      "  num_attention_heads ............................. 16\n",
      "  num_layers ...................................... 24\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... True\n",
      "  openai_gelu ..................................... False\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  report_topk_accuracies .......................... []\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scaled_masked_softmax_fusion .................... False\n",
      "  scaled_upper_triang_masked_softmax_fusion ....... False\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... None\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 969, 30, 1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. BertWordPieceLowerCase\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_cpu_initialization .......................... False\n",
      "  use_one_sent_docs ............................... False\n",
      "  vocab_file ...................................... /tmp/tmpjfjlmirn/666a4a94142a42c0becf3d5f78e95f28_tokenizer.vocab_file\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building BertWordPieceLowerCase tokenizer ...\n",
      " > padded vocab (size: 28996) with 60 dummy tokens (new size: 29056)\n",
      "[NeMo I 2021-08-07 12:40:53 megatron_bert:109] Megatron-lm argparse args: Namespace(DDP_impl='local', adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, adlr_autoresume=False, adlr_autoresume_interval=1000, apply_query_key_layer_scaling=True, apply_residual_connection_post_layernorm=False, attention_dropout=0.1, attention_softmax_in_fp32=False, bert_load=None, bias_dropout_fusion=False, bias_gelu_fusion=False, block_data_path=None, checkpoint_activations=False, checkpoint_num_layers=1, clip_grad=1.0, consumed_train_samples=0, consumed_valid_samples=0, data_impl='infer', data_parallel_size=1, data_path=None, distribute_checkpointed_activations=False, distributed_backend='nccl', eod_mask_loss=False, eval_interval=1000, eval_iters=100, exit_duration_in_mins=None, exit_interval=None, faiss_use_gpu=False, finetune=False, fp16=False, fp16_lm_cross_entropy=False, fp32_allreduce=False, fp32_residual_connection=False, global_batch_size=1, hidden_dropout=0.1, hidden_size=1024, hysteresis=2, ict_head_size=None, ict_load=None, indexer_batch_size=128, indexer_log_interval=1000, init_method_std=0.02, initial_loss_scale=4294967296, layernorm_epsilon=1e-05, lazy_mpu_init=True, load=None, local_rank=None, log_interval=100, loss_scale=None, loss_scale_window=1000, lr=None, lr_decay_iters=None, lr_decay_samples=None, lr_decay_style='linear', lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, make_vocab_size_divisible_by=128, mask_prob=0.15, max_position_embeddings=512, merge_file=None, micro_batch_size=1, min_loss_scale=1.0, min_lr=0.0, mmap_warmup=False, no_load_optim=False, no_load_rng=False, no_save_optim=False, no_save_rng=False, num_attention_heads=16, num_layers=24, num_workers=2, onnx_safe=True, openai_gelu=False, override_lr_scheduler=False, padded_vocab_size=29056, params_dtype=torch.float32, pipeline_model_parallel_size=1, query_in_block_prob=0.1, rampup_batch_size=None, rank=0, report_topk_accuracies=[], reset_attention_mask=False, reset_position_ids=False, save=None, save_interval=None, scaled_masked_softmax_fusion=False, scaled_upper_triang_masked_softmax_fusion=False, seed=1234, seq_length=None, short_seq_prob=0.1, split='969, 30, 1', tensor_model_parallel_size=1, tensorboard_dir=None, titles_data_path=None, tokenizer_type='BertWordPieceLowerCase', train_iters=None, train_samples=None, use_checkpoint_lr_scheduler=False, use_cpu_initialization=True, use_one_sent_docs=False, vocab_file='/tmp/tmpjfjlmirn/666a4a94142a42c0becf3d5f78e95f28_tokenizer.vocab_file', weight_decay=0.01, world_size=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-08-07 12:40:57 megatron_bert:185] Megatron-lm checkpoint version not found. Setting checkpoint_version to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-07 12:40:57 megatron_bert:192] Checkpoint loaded from from /root/.cache/torch/megatron/biomegatron-bert-345m-cased\n",
      "[NeMo I 2021-08-07 12:41:00 modelPT:434] Model TokenClassificationModel was successfully restored from nemo_experiments/token_classification_model/2021-08-07_12-34-13/checkpoints/token_classification_model.nemo.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model by restoring from the .nemo checkpoint\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "LATEST_MODEL = nemo_model_paths[0]\n",
    "model = nemo_nlp.models.TokenClassificationModel.restore_from(LATEST_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with the test set using the `evaluate_from_file` method.  Set the `add_confusion_matrix` to True to get a nice visual representation of how well the model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-07 12:41:02 token_classification_dataset:116] Setting Max Seq length to: 157\n",
      "[NeMo I 2021-08-07 12:41:02 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2021-08-07 12:41:02 data_preprocessing:301] Min: 4 |                  Max: 157 |                  Mean: 37.204255319148935 |                  Median: 35.0\n",
      "[NeMo I 2021-08-07 12:41:02 data_preprocessing:307] 75 percentile: 46.00\n",
      "[NeMo I 2021-08-07 12:41:02 data_preprocessing:308] 99 percentile: 94.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-08-07 12:41:02 token_classification_dataset:145] 0 are longer than 157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-07 12:41:02 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2021-08-07 12:41:02 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2021-08-07 12:41:02 token_classification_dataset:150] subtokens: [CLS] C ##luster ##ing of miss ##ense mutations in the at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a gene in a s ##poradic T - cell le ##uka ##emia . [SEP]\n",
      "[NeMo I 2021-08-07 12:41:02 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2021-08-07 12:41:02 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2021-08-07 12:41:02 token_classification_dataset:153] subtokens_mask: 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "DATA_DIR = '/dli/task/data/NCBI_ner-3'\n",
    "OUTPUT_DIR = '/dli/task/nemo_experiments/token_classification_model/logs'\n",
    "model.evaluate_from_file(\n",
    "    text_file=os.path.join(DATA_DIR, 'text_test.txt'),\n",
    "    labels_file=os.path.join(DATA_DIR, 'labels_test.txt'),\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    add_confusion_matrix=True,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should look something like:\n",
    "\n",
    "```\n",
    "[NeMo I 2021-06-29 00:42:16 token_classification_model:499]                  precision    recall  f1-score   support\n",
    "    \n",
    "    O (label id: 0)     0.9958    0.9910    0.9934     22450\n",
    "    B (label id: 1)     0.8886    0.9135    0.9009       960\n",
    "    I (label id: 2)     0.8724    0.9374    0.9038      1087\n",
    "    \n",
    "           accuracy                         0.9856     24497\n",
    "          macro avg     0.9189    0.9473    0.9327     24497\n",
    "       weighted avg     0.9861    0.9856    0.9858     24497\n",
    "\n",
    "\n",
    "The final confusion matrix visualization shows a bright diagonal, indicating that the predicted label matched the true label with high accuracy for all the label types (IOB).\n",
    "```\n",
    "\n",
    "<img src=\"images/ner_confusion_matrix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.4 Inference\n",
    "To run inference on a list of queries, use the same model already loaded with the `add_predictions` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"Clustering of missense mutations in the ataxia - telangiectasia gene in a sporadic T - cell leukaemia . \",\n",
    "    \"Ataxia - telangiectasia ( A - T ) is a recessive multi - system disorder caused by mutations in the ATM gene at 11q22 - q23 ( ref . 3 ) . \",\n",
    "    \"The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A - T patients and has long been associated with chromosomal instability . \",\n",
    "    \"By analysing tumour DNA from patients with sporadic T - cell prolymphocytic leukaemia ( T - PLL ) , a rare clonal malignancy with similarities to a mature T - cell leukaemia seen in A - T , we demonstrate a high frequency of ATM mutations in T - PLL . \",\n",
    "    \"In marked contrast to the ATM mutation pattern in A - T , the most frequent nucleotide changes in this leukaemia were missense mutations . \",\n",
    "    \"These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM - related proteins in mouse , yeast and Drosophila . \",\n",
    "    \"The resulting amino - acid substitutions are predicted to interfere with ATP binding or substrate recognition . \",\n",
    "    \"Two of seventeen mutated T - PLL samples had a previously reported A - T allele . \",\n",
    "    \"In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia . \",\n",
    "    \"Occasional missense mutations in ATM were also found in tumour DNA from patients with B - cell non - Hodgkins lymphomas ( B - NHL ) and a B - NHL cell line . \"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.add_predictions(queries, output_file='predictions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat predictions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've mastered NeMo and learned:\n",
    "* How to build a named entity recognizer\n",
    "* How to apply a domain-specific model\n",
    "* How to test an NER model with queries from a checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
